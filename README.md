# ğŸ¤– Universal Reddit Scraper Suite

[![Docker Build & Publish](https://github.com/ksanjeev284/reddit-universal-scraper/actions/workflows/docker-publish.yml/badge.svg)](https://github.com/ksanjeev284/reddit-universal-scraper/actions/workflows/docker-publish.yml)

A **full-featured** Reddit scraper suite with analytics dashboard, sentiment analysis, scheduled scraping, notifications, and more!

## âœ¨ Features

| Feature | Description |
|---------|-------------|
| ğŸ“Š **Full Scraping** | Posts, comments, images, videos, galleries |
| ğŸ“ˆ **Analytics Dashboard** | Beautiful Streamlit web UI |
| ğŸ˜€ **Sentiment Analysis** | Analyze post/comment sentiment |
| â˜ï¸ **Keyword Extraction** | Generate word clouds |
| ğŸ” **Search & Filter** | Query scraped data with filters |
| ğŸ“… **Scheduled Scraping** | Cron-style job scheduling |
| ğŸ“§ **Notifications** | Discord & Telegram alerts |
| ğŸ—„ï¸ **SQLite Database** | Structured data storage |
| ğŸ“¤ **Multiple Exports** | CSV, JSON, Excel |

## ğŸš€ Quick Start

```bash
# Install dependencies
pip install -r requirements.txt

# Scrape a subreddit (posts + media + comments)
python main.py delhi --mode full --limit 100

# Launch analytics dashboard
python main.py --dashboard
```

## ğŸ“– Usage Guide

### ğŸ”„ Scraping Modes

```bash
# Full scrape with everything
python main.py delhi --mode full --limit 100

# History only (no media/comments - faster)
python main.py delhi --mode history --limit 500

# Live monitor (checks every 5 min)
python main.py delhi --mode monitor

# Scrape a user's posts
python main.py spez --user --mode full --limit 50

# Skip media or comments
python main.py delhi --mode full --no-media --limit 200
python main.py delhi --mode full --no-comments --limit 200
```

### ğŸ“Š Analytics Dashboard

```bash
# Launch the web dashboard
python main.py --dashboard

# Opens at http://localhost:8501
```

**Dashboard Features:**
- ğŸ“ˆ Post statistics & charts
- ğŸ˜€ Sentiment analysis
- â˜ï¸ Keyword extraction
- ğŸ” Search & filter interface
- ğŸ“¤ Export data

### ğŸ” Search Data

```bash
# Search all scraped data
python main.py --search "credit card"

# Search with filters
python main.py --search "laptop" --min-score 100
python main.py --search "advice" --author username
python main.py --search "help" --subreddit delhi
```

### ğŸ˜€ Analytics

```bash
# Run sentiment analysis
python main.py --analyze delhi --sentiment

# Extract top keywords
python main.py --analyze delhi --keywords
```

### ğŸ“… Scheduled Scraping

```bash
# Scrape every 60 minutes
python main.py --schedule delhi --every 60

# Scrape with options
python main.py --schedule delhi --every 30 --mode full --limit 50
```

### ğŸ“§ Notifications (Discord/Telegram)

**Discord:**
```bash
python main.py delhi --mode monitor --discord-webhook "YOUR_WEBHOOK_URL"
```

**Telegram:**
```bash
python main.py delhi --mode monitor \
  --telegram-token "YOUR_BOT_TOKEN" \
  --telegram-chat "YOUR_CHAT_ID"
```

## ğŸ“ Project Structure

```
reddit-scraper/
â”œâ”€â”€ main.py              # Main CLI entry point
â”œâ”€â”€ config.py            # Configuration settings
â”œâ”€â”€ analytics/           # Sentiment & keyword analysis
â”‚   â””â”€â”€ sentiment.py
â”œâ”€â”€ alerts/              # Discord & Telegram notifications
â”‚   â””â”€â”€ notifications.py
â”œâ”€â”€ dashboard/           # Streamlit web UI
â”‚   â””â”€â”€ app.py
â”œâ”€â”€ export/              # Database & export functions
â”‚   â””â”€â”€ database.py
â”œâ”€â”€ scheduler/           # Cron-style scheduling
â”‚   â””â”€â”€ cron.py
â”œâ”€â”€ search/              # Search & filter engine
â”‚   â””â”€â”€ query.py
â””â”€â”€ data/                # Scraped data
    â””â”€â”€ r_subreddit/
        â”œâ”€â”€ posts.csv
        â”œâ”€â”€ comments.csv
        â””â”€â”€ media/
            â”œâ”€â”€ images/
            â””â”€â”€ videos/
```

## ğŸ“Š Data Output

### posts.csv
| Column | Description |
|--------|-------------|
| id | Reddit post ID |
| title | Post title |
| author | Username |
| score | Net upvotes |
| num_comments | Comment count |
| post_type | text/image/video/gallery |
| selftext | Post body |
| flair | Post flair |
| is_nsfw | NSFW flag |
| created_utc | Timestamp |

### comments.csv
| Column | Description |
|--------|-------------|
| comment_id | Comment ID |
| post_permalink | Parent post |
| author | Username |
| body | Comment text |
| score | Upvotes |
| depth | Nesting level |

## ğŸ³ Docker

```bash
# Build
docker build -t reddit-scraper .

# Full scrape
docker run -v $(pwd)/data:/app/data reddit-scraper delhi --mode full --limit 100

# Monitor mode
docker run -d -v $(pwd)/data:/app/data reddit-scraper delhi --mode monitor
```

## âš™ï¸ Configuration

Edit `config.py` or use environment variables:

```bash
export DISCORD_WEBHOOK_URL="https://discord.com/api/webhooks/..."
export TELEGRAM_BOT_TOKEN="123456:ABC..."
export TELEGRAM_CHAT_ID="987654321"
```

## ğŸ“œ License

MIT License - Feel free to use, modify, and distribute.

## ğŸ¤ Contributing

Pull requests welcome! For major changes, please open an issue first.
